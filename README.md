# robots_txt_analyzer :space_invader:
Программа для анализа файлов robots.txt    
У многих сайтов есть специальный файл robots.txt. Задача этого файла донести до "поисковых" роботов 
(программ, которые шерстят интернет и запоминают его состояние) указать, какие страницы можно запоминать, а какие нет. 
Пример:
Disallow: /search
Allow: /search/about
Allow: /search/static
Allow: /search/howsearchworks
В данном случае инструкция Allow указывает на разрешенную для "индексации" страницу, а Disallow - на запрещенную. 
Остальные инструкции можно игнорировать. 
____
**Задача:**    
Необходимо написать класс RobotsTxtAnalyzer, задачка которого состоит в следующием.    
:white_check_mark: 1. Запросить файл robots.txt у указанного ресурса (например ресурс - google.com -> файл https://google.com/robots.txt)    
:white_check_mark: 2. Прочитать содержимое файла, и составить статистику - количество разрешающих и запрещающих инструкций. 
Так же стоит получить информацию из заголовков ответа, когда редактировался данный файл (заголовок ответа Last-Modified). 
Заголовка может не быть. Тогда оставить поле пустым. При выходе из програмы обновленные данные сохранить в тот же файл.    
:white_check_mark: 3. Полученную информацию записать в JSON-файл, указанный пользователем.
Важно чтобы в одном JSON-файле можно было хранить статистику для нескольких файлов.    
     
**Доп задачи:**    
:white_check_mark: 1. Если указанный ресурс ранее запрашивался, не обновлять статистику, если его актуальное время изменения 
не изменилось с предыдущего раза Для этого запрос все таки придется выполнить (Last-Modified).    
:white_check_mark: 2. Для "предварительного" запроса не скачивать содержимое файла     
:white_check_mark: 3. Реализовать работу класса через контекстный менеджер (загрузка и сохранение данных статистики)
